{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating likelihood ratios with calibrated classifiers\n",
    "\n",
    "Gilles Louppe, January 2016.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy problem\n",
    "\n",
    "Let us consider two 1D distributions $p_0$ and $p_1$ for which we want to approximate the ratio $r(x) = \\frac{p_0(x)}{p_1(x)}$ of their densities.\n",
    "\n",
    "- $p_1$ is defined as a mixture of two gaussians;\n",
    "- $p_0$ is defined as a mixture of the same two gaussians + a bump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from carl.distributions import Normal\n",
    "from carl.distributions import Mixture\n",
    "\n",
    "components = [\n",
    "    Normal(mu=-2.0, sigma=0.75, random_state=0),   # c0\n",
    "    Normal(mu=0.0, sigma=2.0, random_state=1),     # c1\n",
    "    Normal(mu=1.0, sigma=0.5, random_state=2)      # c2 (bump)\n",
    "]\n",
    "\n",
    "bump_coefficient = 0.05\n",
    "g = theano.shared(bump_coefficient) \n",
    "p0 = Mixture(components=components, weights=[0.5 - g / 2., 0.5 - g / 2., g], random_state=10)\n",
    "p1 = Mixture(components=components[:2], weights=[0.5, 0.5], random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for $p_0$, weights are all tied together through the Theano shared variable `g`. This means that changes to the value stored in `g` also automatically change the weight values and the resulting mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_true = p0.rvs(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reals = np.linspace(-5, 5, num=1000)\n",
    "plt.plot(reals, p0.pdf(reals.reshape(-1, 1)), label=r\"$p(x|\\theta_0)$\", color=\"b\")\n",
    "plt.plot(reals, p1.pdf(reals.reshape(-1, 1)), label=r\"$p(x|\\theta_1)$\", color=\"r\")\n",
    "plt.hist(X_true[:, 0], bins=100, normed=True, label=\"data\", alpha=0.2, color=\"b\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.legend(loc=\"best\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density ratio estimation\n",
    "\n",
    "The density ratio $r(x)$ can be approximated using calibrated classifiers, either directly by learning to classify $x \\sim p_0$ from $x \\sim p_1$, or by decomposing the ratio of the two mixtures as pairs of simpler density ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from carl.ratios import ClassifierRatio\n",
    "from carl.ratios import DecomposedRatio\n",
    "from carl.learning import CalibratedClassifierCV\n",
    "from scipy.stats import chi2, norm\n",
    "\n",
    "# Classifier\n",
    "# from sklearn.linear_model import ElasticNetCV\n",
    "# clf = ElasticNetCV()\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(activation=\"tanh\", hidden_layer_sizes=(10, 10), random_state=0)\n",
    "\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# clf = ExtraTreesRegressor(n_estimators=250, max_leaf_nodes=15)\n",
    "\n",
    "n_train_samples = 100000\n",
    "n_calibration_samples = 100000*5\n",
    "\n",
    "# No calibration\n",
    "cc_none = ClassifierRatio(base_estimator=clf)\n",
    "cc_none.fit(numerator=p0, denominator=p1, n_samples=n_train_samples)\n",
    "\n",
    "# Calibration + Direct approximation \n",
    "cv = StratifiedShuffleSplit(n_iter=1, test_size=0.75, random_state=0)  # 25% for training, 75% for calibration\n",
    "cc_direct = ClassifierRatio(base_estimator=CalibratedClassifierCV(clf, cv=cv))\n",
    "cc_direct.fit(numerator=p0, denominator=p1, n_samples=n_train_samples)\n",
    "\n",
    "# Calibration + Decomposition of the mixture\n",
    "cc_decomposed = DecomposedRatio(ClassifierRatio(base_estimator=CalibratedClassifierCV(clf, cv=cv)))\n",
    "cc_decomposed.fit(numerator=p0, denominator=p1, n_samples=n_calibration_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `CalibratedClassifierRatio` takes three arguments for controlling its execution:\n",
    "- `base_estimator` specifying the classifier to be used,\n",
    "- `calibration` specifying the calibration algorithm (`\"kde\"`, `\"histogram\"`, or a user-defined distribution-like object),\n",
    "- `cv` specifying how to allocate data for training and calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(reals, -p0.nnlf(reals.reshape(-1, 1))  \n",
    "                +p1.nnlf(reals.reshape(-1, 1)), label=\"True ratio\")\n",
    "\n",
    "plt.plot(reals, cc_none.predict(reals.reshape(-1, 1), log=True), label=\"No calibration\")\n",
    "plt.plot(reals, cc_direct.predict(reals.reshape(-1, 1), log=True), label=\"Calibration\")\n",
    "plt.plot(reals, cc_decomposed.predict(reals.reshape(-1, 1), log=True), label=\"Calibration + Decomposition\")\n",
    "\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.legend(loc=\"best\", prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using density ratios for maximum likelihood fit\n",
    "\n",
    "In the likelihood-free setting, density ratios can be used to find the maximum likelihood estimator $\\theta^* = \\arg \\max_{\\theta} p(D | \\theta)$ by noticing that $\\theta^*$ also maximizes $\\prod_{x \\in D} \\frac{p(x|\\theta)}{p(x|\\theta_1)}$ for some fixed value of $\\theta_1$.\n",
    "\n",
    "As an example, this can be used to find the bump coefficient in $p_1$, as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "n_trials = 1000 \n",
    "mles = []\n",
    "nll = []\n",
    "\n",
    "for i in range(n_trials):  \n",
    "    # Reset\n",
    "    rng = check_random_state(i)\n",
    "    p0.set_params(random_state=rng)\n",
    "    p1.set_params(random_state=rng)\n",
    "    \n",
    "    g.set_value(bump_coefficient)\n",
    "    X_true = p0.rvs(5000)\n",
    "    \n",
    "    # Fit ratio\n",
    "    clf = MLPRegressor(activation=\"tanh\", hidden_layer_sizes=(10, 10), random_state=rng)\n",
    "    cc = DecomposedRatio(ClassifierRatio(base_estimator=CalibratedClassifierCV(clf, cv=cv)))\n",
    "    cc.fit(numerator=p0, denominator=p1, n_samples=n_calibration_samples)\n",
    "\n",
    "    # Inference\n",
    "    def objective(theta):       \n",
    "        g.set_value(theta[0])  # this indirectly updates weights in p1, \n",
    "                               # along with the density ratios computed by cc \n",
    "                               # (without having to retrain the classifiers since \n",
    "                               # g only affects the weights!)\n",
    "\n",
    "        return -np.sum(cc.predict(X_true, log=True))\n",
    "\n",
    "    results = minimize(objective, x0=[0.1], \n",
    "                       constraints=[{'type':'ineq', 'fun': lambda x: x[0]},\n",
    "                                    {'type':'ineq', 'fun': lambda x: 1. - x[0]},])\n",
    "\n",
    "    # add NLL evaluate at true point\n",
    "    nll_mle = results.fun\n",
    "\n",
    "    g.set_value(bump_coefficient)\n",
    "    nll_true = -np.sum(cc.predict(X_true, log=True))\n",
    "    nll.append(2.*(nll_true - nll_mle))\n",
    "    mles.append(results.x[0])\n",
    "    if i%100 ==0 :\n",
    "        print(i)\n",
    "        print(\"nll: \", nll_mle, nll_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(mles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = plt.hist(mles, bins=30, normed=1, alpha=0.2)\n",
    "plt.vlines(bump_coefficient, 0, h[0].max()+5, linestyles=\"dashed\", label=\"true g\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0,3,50)\n",
    "test = np.linspace(0,3,100)\n",
    "counts, bins, patches = plt.hist(2*nll, bins=bins, normed=1, alpha=0.2)\n",
    "plt.plot(test, chi2.pdf(test,df=1))\n",
    "plt.xlabel('-2 log L(g_true)/L(g_mle)')\n",
    "#plt.ylim(1e-2,5)\n",
    "#plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(bins[1:], np.cumsum(counts)*(bins[1]-bins[0]), c='b')\n",
    "plt.plot(bins[1:], chi2.cdf(bins[1:],df=1), c='g')\n",
    "plt.xlabel('-2 log L(g_true)/L(g_mle)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts, bins, patches = plt.hist(np.sqrt(2*nll), bins=bins, normed=1, alpha=0.2)\n",
    "plt.plot(test, 2*norm.pdf(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
